{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "d:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = '../../../../PrevisaoVento/Data/process/cenario1/dados_cenario1.csv'\n",
    "dataframe = pd.read_csv(source_file, delimiter=';', index_col=0, parse_dates=['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day = dataframe[['Data', 'VentoPrevisto', 'Chuva', 'Pressao','Temperatura','Direcao']].groupby('Data').mean()\n",
    "df_day.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "_ = df_day.plot(y='VentoPrevisto', ax=ax2, lw=1, marker='o', color='blue', alpha=0.5)\n",
    "#_ = df_day.plot(y='Temperatura', ax=ax, lw=1, marker='o', color='red', alpha=0.75)\n",
    "_ = ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day['V_d1'] = df_day['VentoPrevisto'].shift(-1)\n",
    "df_day['V_l1'] = df_day['VentoPrevisto'].shift(1)\n",
    "df_day['V_l2'] = df_day['VentoPrevisto'].shift(2)\n",
    "df_day['V_l3'] = df_day['VentoPrevisto'].shift(3)\n",
    "df_day['V_l4'] = df_day['VentoPrevisto'].shift(4)\n",
    "df_day['V_l5'] = df_day['VentoPrevisto'].shift(5)\n",
    "df_day['V_l6'] = df_day['VentoPrevisto'].shift(6)\n",
    "df_day['V_l7'] = df_day['VentoPrevisto'].shift(7)\n",
    "\n",
    "df_day['C_l1'] = df_day['Chuva'].shift(1)\n",
    "df_day['C_l2'] = df_day['Chuva'].shift(2)\n",
    "df_day['C_l3'] = df_day['Chuva'].shift(3)\n",
    "df_day['C_l4'] = df_day['Chuva'].shift(4)\n",
    "df_day['C_l5'] = df_day['Chuva'].shift(5)\n",
    "df_day['C_l6'] = df_day['Chuva'].shift(6)\n",
    "df_day['C_l7'] = df_day['Chuva'].shift(7)\n",
    "\n",
    "df_day['D_l1'] = df_day['Direcao'].shift(1)\n",
    "df_day['D_l2'] = df_day['Direcao'].shift(2)\n",
    "df_day['D_l3'] = df_day['Direcao'].shift(3)\n",
    "df_day['D_l4'] = df_day['Direcao'].shift(4)\n",
    "df_day['D_l5'] = df_day['Direcao'].shift(5)\n",
    "df_day['D_l6'] = df_day['Direcao'].shift(6)\n",
    "df_day['D_l7'] = df_day['Direcao'].shift(7)\n",
    "\n",
    "df_day['P_l1'] = df_day['Pressao'].shift(1)\n",
    "df_day['P_l2'] = df_day['Pressao'].shift(2)\n",
    "df_day['P_l3'] = df_day['Pressao'].shift(3)\n",
    "df_day['P_l4'] = df_day['Pressao'].shift(4)\n",
    "df_day['P_l5'] = df_day['Pressao'].shift(5)\n",
    "df_day['P_l6'] = df_day['Pressao'].shift(6)\n",
    "df_day['P_l7'] = df_day['Pressao'].shift(7)\n",
    "\n",
    "df_day['T_l1'] = df_day['Temperatura'].shift(1)\n",
    "df_day['T_l2'] = df_day['Temperatura'].shift(2)\n",
    "df_day['T_l3'] = df_day['Temperatura'].shift(3)\n",
    "df_day['T_l4'] = df_day['Temperatura'].shift(4)\n",
    "df_day['T_l5'] = df_day['Temperatura'].shift(5)\n",
    "df_day['T_l6'] = df_day['Temperatura'].shift(6)\n",
    "df_day['T_l7'] = df_day['Temperatura'].shift(7)\n",
    "\n",
    "complete_cases = ~df_day.isna().any(axis=1)\n",
    "dfn = df_day[complete_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.head(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_atual = datetime.datetime.now()\n",
    "print(data_atual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "n_test_samples = 720\n",
    "n_validation_samples = 720\n",
    "random_seed = 32\n",
    "\n",
    "# SET THE RANDOM SEED\n",
    "numpy.random.seed(random_seed)\n",
    "tensorflow.random.set_seed(random_seed)\n",
    "\n",
    "x_cols = ['VentoPrevisto', 'V_l1','V_l2','V_l3','V_l4','V_l5','V_l6','V_l7',  'C_l1','C_l2','C_l3','C_l4','C_l5','C_l6','C_l7',  'D_l1','D_l2','D_l3','D_l4','D_l5','D_l6','D_l7',  'P_l1','P_l2','P_l3','P_l4','P_l5','P_l6','P_l7', 'T_l1','T_l2','T_l3','T_l4','T_l5','T_l6','T_l7']\n",
    "y_cols = ['V_d1']\n",
    "\n",
    "# SAMPLING\n",
    "X_trn = dfn.iloc[:-(n_validation_samples + n_test_samples)][x_cols].to_numpy()\n",
    "X_val = dfn.iloc[-(n_validation_samples + n_test_samples):-n_test_samples][x_cols].to_numpy()\n",
    "X_tst = dfn[x_cols].to_numpy()\n",
    "Y_trn = dfn.iloc[:-(n_validation_samples + n_test_samples)][y_cols].to_numpy()\n",
    "Y_val = dfn.iloc[-(n_validation_samples + n_test_samples):-n_test_samples][y_cols].to_numpy()\n",
    "\n",
    "# SCALING X\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler = X_scaler.fit(X_trn)\n",
    "X_trn_scaled = X_scaler.transform(X_trn)\n",
    "X_val_scaled = X_scaler.transform(X_val)\n",
    "X_tst_scaled = X_scaler.transform(X_tst)\n",
    "\n",
    "# SCALING Y\n",
    "Y_scaler = StandardScaler()\n",
    "Y_scaler = Y_scaler.fit(Y_trn)\n",
    "Y_trn_scaled = Y_scaler.transform(Y_trn)\n",
    "Y_val_scaled = Y_scaler.transform(Y_val)\n",
    "\n",
    "# DECLARE NET\n",
    "max_neurons = 20\n",
    "PATIENCE = 250\n",
    "MAX_EPOCHS = 10*PATIENCE\n",
    "LEARNING_RATE = 0.01\n",
    "HIDDEN_ACTIVATION = 'sigmoid'\n",
    "MODEL_LOSS = 'val_loss'\n",
    "MIN_IMPROVEMENT = 0.025\n",
    "\n",
    "best_loss = 1000.0\n",
    "best_val_loss = 1000.0\n",
    "training_result_list = []\n",
    "for n_neurons in numpy.arange(max_neurons) + 1:\n",
    "    \n",
    "    inputs = keras.Input(shape=(X_trn_scaled.shape[1]))  # Define input (features) layer\n",
    "    hidden = keras.layers.Dense(n_neurons, activation=HIDDEN_ACTIVATION)(inputs) # Define first dense layer , use_bias=True\n",
    "    outputs = keras.layers.Dense(Y_trn_scaled.shape[1], activation='linear')(hidden) #'linear'\n",
    "    \n",
    "    #ativacoes: sigmoid (Sigmoide)*, tanh (Tangente Hiperbólica) *, relu (Unidade Linear Retificada) *, elu **(Unidade Linear Exponencial).\n",
    "    #           softmax*, selu*, softsign , exponential\n",
    "    # use_bias:True  >> tf.keras.layers.Dense(units,activation=None,use_bias=True,\n",
    "        \n",
    "    model_name = 'FORECAST_{}_{}'.format(HIDDEN_ACTIVATION, n_neurons)\n",
    "    my_net = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "    \n",
    "    #* otimizacao = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    # otimizacao = keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9)\n",
    "    # otimizacao = tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE, momentum=0.9)\n",
    "    my_net.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
    "   \n",
    "    \n",
    "    es_callback = EarlyStopping(monitor='val_loss', mode='min', patience=PATIENCE, restore_best_weights=True)\n",
    "    history = my_net.fit(X_trn_scaled, Y_trn_scaled, epochs=MAX_EPOCHS, validation_data=(X_val_scaled, Y_val_scaled), batch_size=X_trn_scaled.shape[0], callbacks=[es_callback], verbose=0)\n",
    "    \n",
    "    model_loss = history.history['loss'][-1]\n",
    "    model_val_loss = history.history['val_loss'][-1]\n",
    "    epochs = len(history.history['val_loss'])\n",
    "\n",
    "    has_improved_val_loss = False\n",
    "    loss_improvement = 1.0 - model_val_loss/best_val_loss\n",
    "\n",
    "    if loss_improvement >= MIN_IMPROVEMENT:\n",
    "        is_best_net = ((model_loss <= best_loss) & (model_val_loss < best_val_loss))\n",
    "\n",
    "        if is_best_net:\n",
    "                best_loss = model_loss\n",
    "                best_val_loss = model_val_loss\n",
    "                best_net = my_net\n",
    "                best_history = history\n",
    "                best_text = '***BEST NET!***'\n",
    "                improvement_text = '{:.0f}% GAIN'.format(100.0*loss_improvement)\n",
    "                best_name = model_name\n",
    "                best_epochs = epochs\n",
    "        else:\n",
    "            best_text = ''\n",
    "            improvement_text = ''\n",
    "    else:\n",
    "        print('EXTRA NEURON DIDNT IMPROVE NET {:.0f}% GAIN'.format(100.0*loss_improvement))\n",
    "        break\n",
    "    \n",
    "    result_dict = {'model_name': model_name, 'epochs': epochs, 'model_loss': model_loss, 'model_val_loss': model_val_loss, 'best_net':is_best_net}\n",
    "    \n",
    "    \n",
    "    training_result_list.append(result_dict)\n",
    "    print('{} RESULTS > EPOCHS: {} LOSS: {:.3f} VAL_LOSS: {:.3f} {} {}'.format(model_name, epochs, model_loss, model_val_loss, best_text, improvement_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_est_scaled = best_net.predict(X_tst_scaled)\n",
    "\n",
    "Y_est = Y_scaler.inverse_transform(Y_est_scaled)\n",
    "\n",
    "\n",
    "df_day.loc[complete_cases, ['Y_est']] = numpy.around(Y_est,3)\n",
    "df_day['residual'] = df_day['Y_est'] - df_day['V_d1']\n",
    "df_day['rabs'] = df_day['residual'].abs()\n",
    "\n",
    "\n",
    "model_r2_score = r2_score(df_day.loc[complete_cases, 'V_d1'].to_numpy(), df_day.loc[complete_cases, 'Y_est'].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_atual = datetime.datetime.now()\n",
    "print(data_atual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111)\n",
    "_ = df_day.plot(y='V_d1', ax=ax, lw=1, marker='o', alpha=0.5)\n",
    "_ = df_day.plot(y='Y_est', ax=ax, lw=1, marker='o', alpha=0.75, color='green')\n",
    "_ = ax.grid()\n",
    "_ = ax.set_title('MODELO {} R^2: {:.0f}%'.format(best_name, 100.0*model_r2_score), size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_day.sort_values('rabs', ascending=True)\n",
    "index_janine = df_day[\"V_d1\"] > 10\n",
    "df_day.loc[index_janine,[\"Y_est\",\"V_d1\",\"residual\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_day.head(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../../../../PrevisaoVento/Data/process/cenario1/predicao_cenario1.csv'\n",
    "df_day.to_csv(output_file, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Instrução para exportação do modelo da rede neural</3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# ** EXPOTACAO DO MODELO\n",
    "model_file = '../../../../PrevisaoVento/Code/Model/modeloC1_sigmoid_previsao.pkl'\n",
    "tensorflow.keras.models.save_model(best_net, model_file)\n",
    "\n",
    "\n",
    "# ** IMPORTACAO DO MODELO\n",
    "# model = load_model(model_file)\n",
    "\n",
    "# ** TESTE DO MODELO IMPORTADO\n",
    "# model.predict(numpy.array([[1, 10,11,13,  10,12,13 ,   90,91,90,  800,880,870  , 26,28,29]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
